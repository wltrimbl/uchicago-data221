{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo:  gaussian mixture model for one-dimensional data;\n",
    "# find maximum-likelihood parameters by writing a loss function\n",
    "# and optimizing it.\n",
    "# Fit constant model (SSE) that maximimzes SSE - likelihood of the data\n",
    "# Fit normal model (mean, stddev) that maximize likelihood of the data\n",
    "# Fit 50/50 gaussian mixture model to maximize likelihood of the data\n",
    "# construct 2d contour maps of the likelihood function as a function of mu1, mu2, sigma1, sigma2\n",
    "# Examine simple 1d Bayesian classifier for this 1d dataset.\n",
    "# Construct an ROC curve for the above Bayesian classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I load in a much-loved dataset from 1888, Francis Galton's heights of 900\n",
    "# adults and their parents.\n",
    "\n",
    "galton = pd.read_csv(\"../data/galton.csv\")\n",
    "galton.head()\n",
    "y = galton.childHeight.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton.family.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(galton.index, galton.childHeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(galton.index, galton.father)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(galton.index, galton.mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(galton.father, galton.mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(galton.father, galton.mother, bins=[np.arange(62.5,77.5, 1), np.arange(57.5, 71, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(galton.childHeight, bins=np.arange(55, 80) + .5) \n",
    "plt.xlabel(\"Height in inches\")\n",
    "plt.ylabel(\"Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure I can use optimization, let's try a test.\n",
    "\n",
    "$$ LOSS_{SSE}(\\theta; y)  = \\sum (y-\\theta)^2 $$\n",
    "\n",
    "$$ \\hat{\\theta} = argmin_{\\theta} \\ \\  LOSS_{SSE} (\\theta; y) $$\n",
    "\n",
    "If I wanted to replace all my y with a constant,\n",
    "what number gives me the smallest summed error?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I define an extremely simple function.\n",
    "# This function sums the squared-differences between\n",
    "# each value in y and the paramter theta.\n",
    "def LOSS_SSE( theta ):\n",
    "    assert len(theta) == 1\n",
    "    return np.sum( (y - theta)**2 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_SSE([64]) , LOSS_SSE([65]), LOSS_SSE([67]),  LOSS_SSE([68]),  LOSS_SSE([69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetahat = minimize(LOSS_SSE, 0)\n",
    "thetahat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare victory, the value 66.75 inches minimizes the sum-squared differences from all 946 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slightly more elaborate model: SUM normal logpdf  ( data,  mean,  stddev )  \n",
    "def LOSS_NORMAL(parameter):\n",
    "    return np.sum( -scipy.stats.norm.logpdf( y, loc=parameter[0], scale=parameter[1])  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_NORMAL([66.7,3.5]), LOSS_NORMAL([66.7,4.5]), LOSS_NORMAL([66.7,5.5]), LOSS_NORMAL([66.7,8.5]), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it look like it has a minimum ? \n",
    "LOSS_NORMAL([86.7,3.5]), LOSS_NORMAL([66.7,3.5]), LOSS_NORMAL([46.7,3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run library optimization function\n",
    "minimize(LOSS_NORMAL, [66, 3.5], method=\"BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare victory, now we have a mean and standard deviation that maximize the likelihood\n",
    "of the data given a normal distribution with location, scale parameters 66.745 and 3.577."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait just one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm. Right.  I have used a machine gun to shoot a squirrel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Normal distribution fit and the data histogram:\n",
    "x = np.arange(55,80,.1)\n",
    "yhat_g = scipy.stats.norm.pdf(x, loc=66.7459, scale=3.577)\n",
    "plt.hist(y, bins=np.arange(55, 80))\n",
    "plt.plot(x,yhat_g *900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some insight into this likelihood function, \n",
    "$$ \\textrm{norm.logpdf}(x, mu, sigma) = c -  \\log \\sigma + {(x - \\mu)^2 \\over 2 \\sigma^2} $$\n",
    "Let's evaluate it on a 2d grid in $\\mu$ and $\\sigma$ and make contour plots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to evaluate our LOSS_NORMAL function on a grid\n",
    "xgrid = np.arange(55,80,2) \n",
    "ygrid = np.arange(0.5, 20.0, 0.5)\n",
    "xax, yax = np.meshgrid(xgrid, ygrid, indexing=\"ij\")\n",
    "z = np.zeros(xax.shape)\n",
    "print(xax.shape, yax.shape, z.shape)\n",
    "for i in range(len(xgrid)):\n",
    "    for j in range(len(ygrid)):\n",
    "        z[i,j]= LOSS_NORMAL((xgrid[i], ygrid[j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arright, I can't find how to plot the axes right without plt.contour and plt.contourf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(xax, yax, np.log(z), levels=30)\n",
    "plt.xlabel(\"posterior mean\")\n",
    "\n",
    "plt.ylabel(\"posterior std\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It has an optimum.  (This is good.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You didn't really need a loss function to find mean and standard deviation of a\n",
    "collection of points.  \n",
    "\n",
    "Now consider this model:\n",
    "\n",
    "$$ P(x, \\mu_1, \\sigma_1, \\mu_2, \\sigma_2) = {1\\over 2} \\mathcal{N} (x; \\mu_1, \\sigma_1) + \n",
    "{1\\over 2} \\mathcal{N} (x; \\mu_2, \\sigma_2)  $$\n",
    "\n",
    "This is a mixture of two Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we construct the sum over all the data points of the log of two normal pdfs:\n",
    "\n",
    "def LOSS_NORMAL2(parameter2):\n",
    "    '''Sums likelihoods over y (assumed already defined) \n",
    "    for sum-of-two-equally-weighted-normal-distributions\n",
    "    with paramters mu_1, sigma_1, mu_2, and sigma_2.'''\n",
    "    assert len(parameter2) ==4  # throw an error if parameter2 has the wrong type\n",
    "    return np.sum( -np.log(\n",
    "                           scipy.stats.norm.pdf( y, loc=parameter2[0], scale=parameter2[1]) +\n",
    "                           scipy.stats.norm.pdf( y, loc=parameter2[2], scale=parameter2[3]) ))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_NORMAL2([60,3.5,70,3.5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to convince myself the sign is right.. \n",
    "LOSS_NORMAL2([0,3.5,70,3.5]), LOSS_NORMAL2([60,3.5,70,3.5]), LOSS_NORMAL2([120,3.5,70,3.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(LOSS_NORMAL2, [63, 2, 70, 2], method=\"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now evaluate this on a grid of len(xgrid2)x len(ygrid2) to make a contour plot\n",
    "xgrid2 = np.arange(55,80,2) \n",
    "ygrid2 = np.arange(55,80,2)\n",
    "xax2, yax2 = np.meshgrid(xgrid2, ygrid2, indexing=\"ij\")\n",
    "z2 = np.zeros(xax2.shape)\n",
    "print(xax2.shape, yax2.shape, z.shape)\n",
    "for i in range(len(xgrid2)):\n",
    "    for j in range(len(ygrid2)):\n",
    "        z2[i,j]= LOSS_NORMAL2((xgrid2[i],2.27,  ygrid2[j], 2.48))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(xax2, yax2, np.log(z2), levels=30)\n",
    "plt.xlabel(\"Mean 1 parameter\")\n",
    "plt.ylabel(\"Mean 2 parameter\")\n",
    "yax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is, believe it or not, reassuring.  There are two equal \n",
    "# optima, one with mean1 = 69 and mean2 = 64, and one with \n",
    "# mean1 = 64 and mean2 = 69; these correspond to switching the \n",
    "# labels between the large and the small groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions for thought:  \n",
    "\n",
    "* The optimizer claimed victory after only 105 evaluations of the loss function.  How many times did I evaluate the loss function to make these contour maps?\n",
    "\n",
    "* The parameter space for my function was (mean1, std1, mean2, std2), that's four dimensions.\n",
    "I plotted a two-dimensional slice, with std1 and std2 held fixed at their optmimum values.\n",
    "Do you think std1 and std1 might be correlated with mean1 and mean2?\n",
    "\n",
    "* There were two paramters (sigma_1 and sigma_2) that were .. \"similar.\"   mean_1 and mean_2 were similar, but it was clear they would chase down different parts of the distribution.  But what does it mean if sigma_1 and sigma_2 are very different?   What would happen to the fitting process if I set sigma_2 = sigma_1 ? \n",
    "\n",
    "* There was one paramter that I failed to parameterize: the mixing coefficient.  In constructing the loss function I implicitly made the weights for normal 1 and normal 2 equal.    Would I get a better or worse fit if I let the algorithm fit the probability ratio between class 1 and class 2?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more contour plotl this one looking at mean_1 and sigma_1 while\n",
    "# the values of mean_2 and sigma_2 are kept fixed.\n",
    "xgrid3 = np.arange(55,80,1) \n",
    "ygrid3 = np.arange(0.5,20,0.25)\n",
    "xax3, yax3 = np.meshgrid(xgrid3, ygrid3, indexing=\"ij\")\n",
    "z3 = np.zeros(xax3.shape)\n",
    "print(xax3.shape, yax3.shape, z.shape)\n",
    "for i in range(len(xgrid3)):\n",
    "    for j in range(len(ygrid3)):\n",
    "        z3[i,j]= LOSS_NORMAL2((xgrid3[i],ygrid3[j] , 69.42351661 , 2.48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(xax3, yax3, np.log(z3), levels=30)\n",
    "plt.xlabel(\"Mean 1 parameter\")\n",
    "plt.ylabel(\"Std 1 parameter\")\n",
    "plt.savefig(\"2d-mu-sigma.png\", dpi=300, bbox_inches=\"tight\")\n",
    "yax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part of the likelihood function, there is a bit of \n",
    "# correlation between mu1 and sigma1. \n",
    "# Correlations are typical...\n",
    "# But these have the effect that optimization of one axis at a time\n",
    "# can be difficult, because the best fit in one direction spoil\n",
    "# the fit in other directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning:  this is a 2d-plane slice through a 4-dimensional likelihood \n",
    "# function, holding mu_2 and sigma_2 constant.  In general, the optimium\n",
    "# for mu_1 and sigma_1 is going to depend on these, so if I wanted to\n",
    "# see the \"real\" joint distribution of mu_1 and sigma_1 I would need\n",
    "# to find the optimium at each point or marginalize (by numerical \n",
    "# integration) to replace mu_2 and sigma_2 with probaiblity-density-informed\n",
    "# expecatation values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P_{marginal}(x) = \\int dy {P_{posterior}(x|y) P_{prior}(y) }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two-paramter fit results were\n",
    "thetahat = [64.08472207,  2.2771205 , 69.42351661,  2.48216295]\n",
    "# and that was for the function \n",
    "yhat = 0.5 * scipy.stats.norm.pdf(x, loc=thetahat[0], scale=thetahat[1]) + 0.5 * scipy.stats.norm.pdf(x, loc=thetahat[2], scale=thetahat[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(55,80,.1)\n",
    "yhat_g = scipy.stats.norm.pdf(x, loc=66.7459, scale=3.577)\n",
    "plt.hist(y,  bins=np.arange(55, 80))\n",
    "plt.plot(x,yhat_g *900)\n",
    "plt.plot(x, yhat*900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So why does the data look like this?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column whose value is 1 if gender is male, 0 otherwise: \n",
    "galton[\"indicator\"] =  galton.gender == \"male\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton.groupby(by=\"indicator\").childHeight.hist(alpha=0.5, bins=np.arange(55, 80))\n",
    "plt.xlabel(\"Child height (in)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the per-category mean and standard devation\n",
    "\n",
    "# This goes by the jargon \"Class-conditional distribution\"\n",
    "# which is to say, \"All the data in class 1\"\n",
    "\n",
    "galton.groupby(by=\"indicator\").childHeight.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And can write down theoretical densities for normal distributions\n",
    "# with the empirical mean and standard devation for the two classes:\n",
    "density1 = scipy.stats.norm.pdf(x, loc=64.103974, scale=2.355653)\n",
    "density2 = scipy.stats.norm.pdf(x, loc=69.234096, scale=2.623905)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(121)\n",
    "plt.hist(y,  bins=np.arange(55, 80))\n",
    "plt.plot(x, np.array([(density1+density2)*450 ]).T ) \n",
    "plt.subplot(122)\n",
    "plt.plot(x, np.array([density1*450, density2*450 ]).T ) \n",
    "galton.groupby(by=\"indicator\").childHeight.hist(alpha=0.5, bins=np.arange(55, 80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best decision I can make is to pick the most likely class for each \n",
    "# value of x\n",
    "x[np.min(np.where(density1<density2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And if I classify everyone shorter than 67.7 as female, \n",
    "# this is how my predictions compare to the original labels:\n",
    "galton[\"classification\"] = galton.childHeight > 67.7 \n",
    "galton.groupby([\"classification\", \"indicator\"]).indicator.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Correct: 773    Incorrect:  161 \n",
    "#  Errors incorrectly classified as M : 32\n",
    "#         incorrectly classified as F: 129 \n",
    "\n",
    "len(galton), galton.indicator.sum(), galton.indicator.sum()/len(galton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall accuracy, adjusted for nothing:\n",
    "773/934"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' rule for inferring probability of class $\\mathcal{C}_k $  ($k$ is the index that counts $r$ classes) given data $x$: \n",
    "\n",
    "$$ P(\\mathcal{C}_k | x) \\propto {P(x| \\mathcal{C}_k) P(\\mathcal{C}_k)}  $$\n",
    "\n",
    "And the probability that $x$ came from class  $\\mathcal{C}_1 $ is\n",
    "\n",
    "$$ P(\\mathcal{C}_1 | x) = {P(x| \\mathcal{C}_1) P(\\mathcal{C}_1) \\over \n",
    "\\displaystyle\\sum_{i=1}^r P(x| \\mathcal{C}_i) P(\\mathcal{C}_i) }  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I want a graph of my model's posterior probability of being\n",
    "# in class 1 or class 2 as a function of x: \n",
    "\n",
    "p_class1 = density1 /(density1 + density2)\n",
    "p_class2 = density2 /(density1 + density2)\n",
    "\n",
    "plt.plot(x,p_class1)\n",
    "plt.plot(x,p_class2) \n",
    "plt.ylabel(\"Posterior probablity of class | x\")\n",
    "plt.xlabel(\"child Height (in)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, np.array([density1*450, density2*450 ]).T ) \n",
    "galton.groupby(by=\"indicator\").childHeight.hist(alpha=0.5, bins=np.arange(55, 80))\n",
    "plt.plot([66.7,66.7], [0,80])\n",
    "plt.xlabel(\"Child height (in)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fpr_fnr(threshold):\n",
    "    a = galton.query(\"(indicator == 0) & childHeight <= @threshold\").childHeight.count()\n",
    "    b = galton.query(\"(indicator == 0) & childHeight > @threshold\").childHeight.count()\n",
    "    c = galton.query(\"(indicator == 1) & childHeight <= @threshold\").childHeight.count()\n",
    "    d = galton.query(\"(indicator == 1) & childHeight > @threshold\").childHeight.count()\n",
    "    print(threshold, a,b,c,d)\n",
    "    return (b / (a+b) , c / (c+d) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_fpr_fnr(62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trange = range(50,80)\n",
    "fprfnr = np.array([find_fpr_fnr(threshold) for threshold in trange])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.plot(fprfnr[:,0], 1-fprfnr[:,1]) \n",
    "for x,y,z in zip (fprfnr[:,0], 1-np.array(fprfnr[:,1]) , list(map(str, trange))):\n",
    "    print (x,y,z)\n",
    "    ax.text(x, y, z)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "plt.step(fprfnr[:,0], 1-fprfnr[:,1]) \n",
    "for x,y,z in zip (fprfnr[:,0], 1-np.array(fprfnr[:,1]) , list(map(str, trange))):\n",
    "    print (x,y,z)\n",
    "    ax.text(x, y, z)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel(\"FPR\", fontsize=14)\n",
    "plt.ylabel(\"TPR\", fontsize=14)\n",
    "plt.title(\"ROC curve for Galton height data / gender\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_fpr_fnr(66.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
